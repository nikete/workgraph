//! Tests for reward recording and performance aggregation.
//!
//! Covers:
//! 1. record_reward writes correct JSON format
//! 2. Multiple rewards for same agent aggregate correctly
//! 3. update_performance on role, objective, and agent independently track context_ids
//! 4. Performance record with 0, 1, and 10+ rewards
//! 5. Reward dimensions scoring (all dimension fields preserved)
//! 6. recalculate_mean_reward with various edge cases

use std::collections::HashMap;
use tempfile::TempDir;

use workgraph::identity::{
    self, Agent, Reward, RewardRef, Lineage, RewardHistory, SkillRef,
};

// ---------------------------------------------------------------------------
// Helpers
// ---------------------------------------------------------------------------

/// Create a test identity dir with one role, one objective, and optionally an agent.
struct TestFixture {
    identity_dir: std::path::PathBuf,
    role_id: String,
    objective_id: String,
    agent_id: String,
    _tmp: TempDir,
}

impl TestFixture {
    fn new() -> Self {
        Self::with_agent(true)
    }

    fn with_agent(create_agent: bool) -> Self {
        let tmp = TempDir::new().unwrap();
        let identity_dir = tmp.path().join("identity");
        identity::init(&identity_dir).unwrap();

        let role = identity::build_role(
            "Test Role",
            "A role for testing rewards.",
            vec![
                SkillRef::Name("rust".to_string()),
                SkillRef::Inline("Write tests".to_string()),
            ],
            "Tested code",
        );
        let role_id = role.id.clone();
        identity::save_role(&role, &identity_dir.join("roles")).unwrap();

        let objective = identity::build_objective(
            "Test Objective",
            "A objective for testing rewards.",
            vec!["Slower pace".to_string()],
            vec!["Untested code".to_string()],
        );
        let objective_id = objective.id.clone();
        identity::save_objective(&objective, &identity_dir.join("objectives")).unwrap();

        let agent_id = identity::content_hash_agent(&role_id, &objective_id);

        if create_agent {
            let agent = Agent {
                id: agent_id.clone(),
                role_id: role_id.clone(),
                objective_id: objective_id.clone(),
                name: "test-agent".to_string(),
                performance: RewardHistory {
                    task_count: 0,
                    mean_reward: None,
                    rewards: vec![],
                },
                lineage: Lineage::default(),
                capabilities: Vec::new(),
                rate: None,
                capacity: None,
                trust_level: Default::default(),
                contact: None,
                executor: "claude".to_string(),
            };
            identity::save_agent(&agent, &identity_dir.join("agents")).unwrap();
        }

        TestFixture {
            identity_dir,
            role_id,
            objective_id,
            agent_id,
            _tmp: tmp,
        }
    }

    fn make_reward(&self, id: &str, task_id: &str, value: f64, with_agent: bool) -> Reward {
        self.make_reward_with_dims(id, task_id, value, with_agent, HashMap::new())
    }

    fn make_reward_with_dims(
        &self,
        id: &str,
        task_id: &str,
        value: f64,
        with_agent: bool,
        dimensions: HashMap<String, f64>,
    ) -> Reward {
        Reward {
            id: id.to_string(),
            task_id: task_id.to_string(),
            agent_id: if with_agent {
                self.agent_id.clone()
            } else {
                String::new()
            },
            role_id: self.role_id.clone(),
            objective_id: self.objective_id.clone(),
            value,
            dimensions,
            notes: format!("Test eval {}", id),
            evaluator: "test-harness".to_string(),
            timestamp: "2025-06-01T12:00:00Z".to_string(),
            model: None, source: "llm".to_string(),
        }
    }
}

// ===========================================================================
// 1. record_reward writes correct JSON format
// ===========================================================================

/// Verify the reward JSON file contains all expected fields with correct values.
#[test]
fn test_record_reward_json_format() {
    let fix = TestFixture::new();

    let mut dimensions = HashMap::new();
    dimensions.insert("correctness".to_string(), 0.92);
    dimensions.insert("completeness".to_string(), 0.85);
    dimensions.insert("efficiency".to_string(), 0.78);
    dimensions.insert("style_adherence".to_string(), 0.91);

    let eval = Reward {
        id: "eval-json-format-1".to_string(),
        task_id: "json-task".to_string(),
        agent_id: fix.agent_id.clone(),
        role_id: fix.role_id.clone(),
        objective_id: fix.objective_id.clone(),
        value: 0.87,
        dimensions: dimensions.clone(),
        notes: "Testing JSON format preservation.".to_string(),
        evaluator: "human-reviewer".to_string(),
        timestamp: "2025-06-15T14:30:00Z".to_string(),
        model: None, source: "llm".to_string(),
    };

    let eval_path = identity::record_reward(&eval, &fix.identity_dir).unwrap();

    // Verify file exists and is valid JSON
    assert!(eval_path.exists());
    let raw_json = std::fs::read_to_string(&eval_path).unwrap();
    let parsed: serde_json::Value = serde_json::from_str(&raw_json).unwrap();

    // Verify all top-level fields
    assert_eq!(parsed["id"], "eval-json-format-1");
    assert_eq!(parsed["task_id"], "json-task");
    assert_eq!(parsed["agent_id"], fix.agent_id.as_str());
    assert_eq!(parsed["role_id"], fix.role_id.as_str());
    assert_eq!(parsed["objective_id"], fix.objective_id.as_str());
    assert_eq!(parsed["value"], 0.87);
    assert_eq!(parsed["notes"], "Testing JSON format preservation.");
    assert_eq!(parsed["evaluator"], "human-reviewer");
    assert_eq!(parsed["timestamp"], "2025-06-15T14:30:00Z");

    // Verify dimensions map preserved
    let dims = parsed["dimensions"].as_object().unwrap();
    assert_eq!(dims.len(), 4);
    assert_eq!(dims["correctness"], 0.92);
    assert_eq!(dims["completeness"], 0.85);
    assert_eq!(dims["efficiency"], 0.78);
    assert_eq!(dims["style_adherence"], 0.91);
}

/// Verify the reward filename uses the expected format: eval-{task_id}-{safe_timestamp}.json
#[test]
fn test_record_reward_filename_format() {
    let fix = TestFixture::new();
    let eval = fix.make_reward("eval-1", "my-task", 0.8, false);

    let eval_path = identity::record_reward(&eval, &fix.identity_dir).unwrap();

    let filename = eval_path.file_name().unwrap().to_str().unwrap();
    // Timestamp "2025-06-01T12:00:00Z" has colons replaced with hyphens
    assert_eq!(filename, "eval-my-task-2025-06-01T12-00-00Z.json");
}

/// Verify that the JSON file can be deserialized back into an identical Reward.
#[test]
fn test_record_reward_round_trip() {
    let fix = TestFixture::new();

    let mut dimensions = HashMap::new();
    dimensions.insert("correctness".to_string(), 1.0);
    dimensions.insert("completeness".to_string(), 0.0);

    let eval = fix.make_reward_with_dims("round-trip", "rt-task", 0.5, true, dimensions);
    let eval_path = identity::record_reward(&eval, &fix.identity_dir).unwrap();

    let loaded = identity::load_reward(&eval_path).unwrap();
    assert_eq!(loaded.id, eval.id);
    assert_eq!(loaded.task_id, eval.task_id);
    assert_eq!(loaded.agent_id, eval.agent_id);
    assert_eq!(loaded.role_id, eval.role_id);
    assert_eq!(loaded.objective_id, eval.objective_id);
    assert_eq!(loaded.value, eval.value);
    assert_eq!(loaded.dimensions, eval.dimensions);
    assert_eq!(loaded.notes, eval.notes);
    assert_eq!(loaded.evaluator, eval.evaluator);
    assert_eq!(loaded.timestamp, eval.timestamp);
}

/// Verify that rewards with empty dimensions map serialize as empty object.
#[test]
fn test_record_reward_empty_dimensions() {
    let fix = TestFixture::new();
    let eval = fix.make_reward("no-dims", "task-no-dims", 0.75, false);

    let eval_path = identity::record_reward(&eval, &fix.identity_dir).unwrap();
    let loaded = identity::load_reward(&eval_path).unwrap();

    assert!(loaded.dimensions.is_empty());
}

// ===========================================================================
// 2. Multiple rewards for same agent aggregate correctly
// ===========================================================================

/// Two rewards for the same agent produce correct cumulative mean_reward.
#[test]
fn test_multiple_rewards_same_agent_avg() {
    let fix = TestFixture::new();

    let eval1 = Reward {
        id: "e1".to_string(),
        task_id: "task-1".to_string(),
        agent_id: fix.agent_id.clone(),
        role_id: fix.role_id.clone(),
        objective_id: fix.objective_id.clone(),
        value: 0.80,
        dimensions: HashMap::new(),
        notes: String::new(),
        evaluator: "test".to_string(),
        timestamp: "2025-06-01T10:00:00Z".to_string(),
        model: None, source: "llm".to_string(),
    };
    let eval2 = Reward {
        id: "e2".to_string(),
        task_id: "task-2".to_string(),
        agent_id: fix.agent_id.clone(),
        role_id: fix.role_id.clone(),
        objective_id: fix.objective_id.clone(),
        value: 0.90,
        dimensions: HashMap::new(),
        notes: String::new(),
        evaluator: "test".to_string(),
        timestamp: "2025-06-01T11:00:00Z".to_string(),
        model: None, source: "llm".to_string(),
    };

    identity::record_reward(&eval1, &fix.identity_dir).unwrap();
    identity::record_reward(&eval2, &fix.identity_dir).unwrap();

    let agent =
        identity::find_agent_by_prefix(&fix.identity_dir.join("agents"), &fix.agent_id).unwrap();

    assert_eq!(agent.performance.task_count, 2);
    let expected = (0.80 + 0.90) / 2.0;
    assert!(
        (agent.performance.mean_reward.unwrap() - expected).abs() < 1e-10,
        "Expected avg {}, got {:?}",
        expected,
        agent.performance.mean_reward
    );
    assert_eq!(agent.performance.rewards.len(), 2);
    assert_eq!(agent.performance.rewards[0].task_id, "task-1");
    assert_eq!(agent.performance.rewards[1].task_id, "task-2");
}

/// Three rewards for the same agent: incremental avg is always recalculated, not accumulated.
#[test]
fn test_three_rewards_incremental_avg() {
    let fix = TestFixture::new();

    let values = [0.60, 0.80, 1.0];
    for (i, &value) in values.iter().enumerate() {
        let eval = Reward {
            id: format!("e{}", i),
            task_id: format!("task-{}", i),
            agent_id: fix.agent_id.clone(),
            role_id: fix.role_id.clone(),
            objective_id: fix.objective_id.clone(),
            value,
            dimensions: HashMap::new(),
            notes: String::new(),
            evaluator: "test".to_string(),
            timestamp: format!("2025-06-01T{}:00:00Z", 10 + i),
            model: None, source: "llm".to_string(),
        };
        identity::record_reward(&eval, &fix.identity_dir).unwrap();
    }

    let agent =
        identity::find_agent_by_prefix(&fix.identity_dir.join("agents"), &fix.agent_id).unwrap();

    assert_eq!(agent.performance.task_count, 3);
    let expected = (0.60 + 0.80 + 1.0) / 3.0;
    assert!(
        (agent.performance.mean_reward.unwrap() - expected).abs() < 1e-10,
        "Expected avg {}, got {:?}",
        expected,
        agent.performance.mean_reward
    );
}

// ===========================================================================
// 3. update_performance on role, objective, and agent independently track context_ids
// ===========================================================================

/// Role stores objective_id as context_id, objective stores role_id as context_id,
/// agent stores role_id as context_id â€” all independently.
#[test]
fn test_context_ids_tracked_independently() {
    let fix = TestFixture::new();

    let eval = Reward {
        id: "ctx-test-1".to_string(),
        task_id: "context-task".to_string(),
        agent_id: fix.agent_id.clone(),
        role_id: fix.role_id.clone(),
        objective_id: fix.objective_id.clone(),
        value: 0.88,
        dimensions: HashMap::new(),
        notes: String::new(),
        evaluator: "test".to_string(),
        timestamp: "2025-06-01T12:00:00Z".to_string(),
        model: None, source: "llm".to_string(),
    };
    identity::record_reward(&eval, &fix.identity_dir).unwrap();

    // Agent's context_id = role_id (identifies which role was used)
    let agent =
        identity::find_agent_by_prefix(&fix.identity_dir.join("agents"), &fix.agent_id).unwrap();
    assert_eq!(
        agent.performance.rewards[0].context_id, fix.role_id,
        "Agent context_id should be the role_id"
    );

    // Role's context_id = objective_id
    let role = identity::load_role(
        &fix.identity_dir
            .join("roles")
            .join(format!("{}.yaml", fix.role_id)),
    )
    .unwrap();
    assert_eq!(
        role.performance.rewards[0].context_id, fix.objective_id,
        "Role context_id should be the objective_id"
    );

    // Objective's context_id = role_id
    let objective = identity::load_objective(
        &fix.identity_dir
            .join("objectives")
            .join(format!("{}.yaml", fix.objective_id)),
    )
    .unwrap();
    assert_eq!(
        objective.performance.rewards[0].context_id, fix.role_id,
        "Objective context_id should be the role_id"
    );
}

/// When the same role is used with different objectives, each reward ref stores
/// the correct objective_id as context_id on the role.
#[test]
fn test_role_tracks_different_objective_context_ids() {
    let tmp = TempDir::new().unwrap();
    let identity_dir = tmp.path().join("identity");
    identity::init(&identity_dir).unwrap();

    let role = identity::build_role("Multi-mot Role", "desc", vec![], "outcome");
    identity::save_role(&role, &identity_dir.join("roles")).unwrap();

    let mot_a = identity::build_objective("Mot A", "first", vec![], vec![]);
    let mot_b = identity::build_objective("Mot B", "second", vec!["compromise".to_string()], vec![]);
    identity::save_objective(&mot_a, &identity_dir.join("objectives")).unwrap();
    identity::save_objective(&mot_b, &identity_dir.join("objectives")).unwrap();

    // Eval with mot_a
    let eval_a = Reward {
        id: "e-a".to_string(),
        task_id: "task-a".to_string(),
        agent_id: String::new(),
        role_id: role.id.clone(),
        objective_id: mot_a.id.clone(),
        value: 0.70,
        dimensions: HashMap::new(),
        notes: String::new(),
        evaluator: "test".to_string(),
        timestamp: "2025-06-01T10:00:00Z".to_string(),
        model: None, source: "llm".to_string(),
    };
    identity::record_reward(&eval_a, &identity_dir).unwrap();

    // Eval with mot_b
    let eval_b = Reward {
        id: "e-b".to_string(),
        task_id: "task-b".to_string(),
        agent_id: String::new(),
        role_id: role.id.clone(),
        objective_id: mot_b.id.clone(),
        value: 0.90,
        dimensions: HashMap::new(),
        notes: String::new(),
        evaluator: "test".to_string(),
        timestamp: "2025-06-01T11:00:00Z".to_string(),
        model: None, source: "llm".to_string(),
    };
    identity::record_reward(&eval_b, &identity_dir).unwrap();

    let updated_role =
        identity::load_role(&identity_dir.join("roles").join(format!("{}.yaml", role.id))).unwrap();
    assert_eq!(updated_role.performance.rewards.len(), 2);
    assert_eq!(
        updated_role.performance.rewards[0].context_id, mot_a.id,
        "First eval should track mot_a as context"
    );
    assert_eq!(
        updated_role.performance.rewards[1].context_id, mot_b.id,
        "Second eval should track mot_b as context"
    );
}

/// When the same objective is used with different roles, each reward ref stores
/// the correct role_id as context_id on the objective.
#[test]
fn test_objective_tracks_different_role_context_ids() {
    let tmp = TempDir::new().unwrap();
    let identity_dir = tmp.path().join("identity");
    identity::init(&identity_dir).unwrap();

    let role_a = identity::build_role("Role A", "first", vec![], "outcome a");
    let role_b = identity::build_role("Role B", "second", vec![], "outcome b");
    identity::save_role(&role_a, &identity_dir.join("roles")).unwrap();
    identity::save_role(&role_b, &identity_dir.join("roles")).unwrap();

    let mot = identity::build_objective("Multi-role Mot", "desc", vec![], vec![]);
    identity::save_objective(&mot, &identity_dir.join("objectives")).unwrap();

    let eval_a = Reward {
        id: "e-ra".to_string(),
        task_id: "task-ra".to_string(),
        agent_id: String::new(),
        role_id: role_a.id.clone(),
        objective_id: mot.id.clone(),
        value: 0.65,
        dimensions: HashMap::new(),
        notes: String::new(),
        evaluator: "test".to_string(),
        timestamp: "2025-06-01T10:00:00Z".to_string(),
        model: None, source: "llm".to_string(),
    };
    let eval_b = Reward {
        id: "e-rb".to_string(),
        task_id: "task-rb".to_string(),
        agent_id: String::new(),
        role_id: role_b.id.clone(),
        objective_id: mot.id.clone(),
        value: 0.95,
        dimensions: HashMap::new(),
        notes: String::new(),
        evaluator: "test".to_string(),
        timestamp: "2025-06-01T11:00:00Z".to_string(),
        model: None, source: "llm".to_string(),
    };

    identity::record_reward(&eval_a, &identity_dir).unwrap();
    identity::record_reward(&eval_b, &identity_dir).unwrap();

    let updated_mot = identity::load_objective(
        &identity_dir
            .join("objectives")
            .join(format!("{}.yaml", mot.id)),
    )
    .unwrap();
    assert_eq!(updated_mot.performance.rewards.len(), 2);
    assert_eq!(
        updated_mot.performance.rewards[0].context_id, role_a.id,
        "First eval should track role_a as context"
    );
    assert_eq!(
        updated_mot.performance.rewards[1].context_id, role_b.id,
        "Second eval should track role_b as context"
    );
}

// ===========================================================================
// 4. Performance record with 0, 1, and 10+ rewards
// ===========================================================================

/// A fresh performance record (0 rewards) has None mean_reward and empty rewards.
#[test]
fn test_performance_zero_rewards() {
    let record = RewardHistory {
        task_count: 0,
        mean_reward: None,
        rewards: vec![],
    };
    assert_eq!(record.task_count, 0);
    assert!(record.mean_reward.is_none());
    assert!(record.rewards.is_empty());
}

/// A fresh performance record round-trips through YAML (0 rewards).
#[test]
fn test_performance_zero_rewards_yaml_roundtrip() {
    let tmp = TempDir::new().unwrap();
    let identity_dir = tmp.path().join("identity");
    identity::init(&identity_dir).unwrap();

    let role = identity::build_role("Fresh", "no evals yet", vec![], "outcome");
    identity::save_role(&role, &identity_dir.join("roles")).unwrap();

    let loaded =
        identity::load_role(&identity_dir.join("roles").join(format!("{}.yaml", role.id))).unwrap();
    assert_eq!(loaded.performance.task_count, 0);
    assert!(loaded.performance.mean_reward.is_none());
    assert!(loaded.performance.rewards.is_empty());
}

/// Performance record with exactly 1 reward.
#[test]
fn test_performance_one_reward() {
    let mut record = RewardHistory {
        task_count: 0,
        mean_reward: None,
        rewards: vec![],
    };

    identity::update_performance(
        &mut record,
        RewardRef {
            value: 0.77,
            task_id: "single-task".to_string(),
            timestamp: "2025-06-01T10:00:00Z".to_string(),
            context_id: "ctx-1".to_string(),
        },
    );

    assert_eq!(record.task_count, 1);
    assert!((record.mean_reward.unwrap() - 0.77).abs() < 1e-10);
    assert_eq!(record.rewards.len(), 1);
    assert_eq!(record.rewards[0].task_id, "single-task");
}

/// Performance record with 10+ rewards accumulates correctly.
#[test]
fn test_performance_ten_plus_rewards() {
    let mut record = RewardHistory {
        task_count: 0,
        mean_reward: None,
        rewards: vec![],
    };

    let values: Vec<f64> = (0..15).map(|i| 0.5 + (i as f64) * 0.03).collect();

    for (i, &value) in values.iter().enumerate() {
        identity::update_performance(
            &mut record,
            RewardRef {
                value,
                task_id: format!("task-{}", i),
                timestamp: format!("2025-06-{:02}T10:00:00Z", i + 1),
                context_id: format!("ctx-{}", i),
            },
        );
    }

    assert_eq!(record.task_count, 15);
    assert_eq!(record.rewards.len(), 15);

    let expected_avg: f64 = values.iter().sum::<f64>() / values.len() as f64;
    assert!(
        (record.mean_reward.unwrap() - expected_avg).abs() < 1e-10,
        "Expected avg {}, got {:?}",
        expected_avg,
        record.mean_reward
    );

    // Verify each reward ref stored correctly
    for (i, eval_ref) in record.rewards.iter().enumerate() {
        assert_eq!(eval_ref.task_id, format!("task-{}", i));
        assert_eq!(eval_ref.context_id, format!("ctx-{}", i));
        assert!((eval_ref.value - values[i]).abs() < 1e-10);
    }
}

/// Full end-to-end: record 12 rewards via record_reward and verify
/// agent, role, and objective all accumulate correctly.
#[test]
fn test_twelve_rewards_end_to_end() {
    let fix = TestFixture::new();

    let values: Vec<f64> = (0..12).map(|i| 0.60 + (i as f64) * 0.03).collect();

    for (i, &value) in values.iter().enumerate() {
        let eval = Reward {
            id: format!("e12-{}", i),
            task_id: format!("task-12-{}", i),
            agent_id: fix.agent_id.clone(),
            role_id: fix.role_id.clone(),
            objective_id: fix.objective_id.clone(),
            value,
            dimensions: HashMap::new(),
            notes: String::new(),
            evaluator: "test".to_string(),
            timestamp: format!("2025-06-{:02}T10:00:00Z", i + 1),
            model: None, source: "llm".to_string(),
        };
        identity::record_reward(&eval, &fix.identity_dir).unwrap();
    }

    let expected_avg = values.iter().sum::<f64>() / values.len() as f64;

    // Agent
    let agent =
        identity::find_agent_by_prefix(&fix.identity_dir.join("agents"), &fix.agent_id).unwrap();
    assert_eq!(agent.performance.task_count, 12);
    assert_eq!(agent.performance.rewards.len(), 12);
    assert!(
        (agent.performance.mean_reward.unwrap() - expected_avg).abs() < 1e-10,
        "Agent avg: expected {}, got {:?}",
        expected_avg,
        agent.performance.mean_reward
    );

    // Role
    let role = identity::load_role(
        &fix.identity_dir
            .join("roles")
            .join(format!("{}.yaml", fix.role_id)),
    )
    .unwrap();
    assert_eq!(role.performance.task_count, 12);
    assert_eq!(role.performance.rewards.len(), 12);
    assert!((role.performance.mean_reward.unwrap() - expected_avg).abs() < 1e-10);

    // Objective
    let mot = identity::load_objective(
        &fix.identity_dir
            .join("objectives")
            .join(format!("{}.yaml", fix.objective_id)),
    )
    .unwrap();
    assert_eq!(mot.performance.task_count, 12);
    assert_eq!(mot.performance.rewards.len(), 12);
    assert!((mot.performance.mean_reward.unwrap() - expected_avg).abs() < 1e-10);

    // All 12 reward files should exist on disk
    let all_evals = identity::load_all_rewards(&fix.identity_dir.join("rewards")).unwrap();
    assert_eq!(all_evals.len(), 12);
}

// ===========================================================================
// 5. Reward dimensions scoring (all dimension fields preserved)
// ===========================================================================

/// All four standard dimension fields are preserved through record + load.
#[test]
fn test_all_dimension_fields_preserved() {
    let fix = TestFixture::new();

    let mut dimensions = HashMap::new();
    dimensions.insert("correctness".to_string(), 0.95);
    dimensions.insert("completeness".to_string(), 0.88);
    dimensions.insert("efficiency".to_string(), 0.72);
    dimensions.insert("style_adherence".to_string(), 0.91);

    let eval =
        fix.make_reward_with_dims("dim-test", "dims-task", 0.87, true, dimensions.clone());

    let eval_path = identity::record_reward(&eval, &fix.identity_dir).unwrap();
    let loaded = identity::load_reward(&eval_path).unwrap();

    assert_eq!(loaded.dimensions.len(), 4);
    for (key, &expected_val) in &dimensions {
        let actual = loaded.dimensions.get(key).unwrap();
        assert!(
            (actual - expected_val).abs() < 1e-10,
            "Dimension '{}': expected {}, got {}",
            key,
            expected_val,
            actual
        );
    }
}

/// Custom/non-standard dimension fields are also preserved.
#[test]
fn test_custom_dimension_fields_preserved() {
    let fix = TestFixture::new();

    let mut dimensions = HashMap::new();
    dimensions.insert("correctness".to_string(), 0.9);
    dimensions.insert("creativity".to_string(), 0.85);
    dimensions.insert("documentation_quality".to_string(), 0.7);
    dimensions.insert("test_coverage".to_string(), 0.95);
    dimensions.insert("performance_impact".to_string(), 0.6);

    let eval = fix.make_reward_with_dims(
        "custom-dims",
        "custom-task",
        0.80,
        false,
        dimensions.clone(),
    );

    let eval_path = identity::record_reward(&eval, &fix.identity_dir).unwrap();
    let loaded = identity::load_reward(&eval_path).unwrap();

    assert_eq!(loaded.dimensions.len(), 5);
    for (key, &expected_val) in &dimensions {
        assert!(
            (loaded.dimensions[key] - expected_val).abs() < 1e-10,
            "Custom dimension '{}' not preserved correctly",
            key
        );
    }
}

/// Dimensions with extreme values (0.0 and 1.0) are preserved.
#[test]
fn test_dimension_extreme_values() {
    let fix = TestFixture::new();

    let mut dimensions = HashMap::new();
    dimensions.insert("correctness".to_string(), 0.0);
    dimensions.insert("completeness".to_string(), 1.0);

    let eval =
        fix.make_reward_with_dims("extreme-dims", "ext-task", 0.5, false, dimensions.clone());

    let eval_path = identity::record_reward(&eval, &fix.identity_dir).unwrap();
    let loaded = identity::load_reward(&eval_path).unwrap();

    assert!((loaded.dimensions["correctness"] - 0.0).abs() < 1e-10);
    assert!((loaded.dimensions["completeness"] - 1.0).abs() < 1e-10);
}

/// Dimensions are independent of the overall value (value is not derived from dimensions).
#[test]
fn test_dimensions_independent_of_value() {
    let fix = TestFixture::new();

    let mut dimensions = HashMap::new();
    dimensions.insert("correctness".to_string(), 0.5);
    dimensions.insert("completeness".to_string(), 0.5);

    // Overall value intentionally different from any dimension average
    let eval =
        fix.make_reward_with_dims("indep-dims", "indep-task", 0.99, false, dimensions.clone());

    let eval_path = identity::record_reward(&eval, &fix.identity_dir).unwrap();
    let loaded = identity::load_reward(&eval_path).unwrap();

    // Value is stored as-is, not recalculated from dimensions
    assert!((loaded.value - 0.99).abs() < 1e-10);
    assert!((loaded.dimensions["correctness"] - 0.5).abs() < 1e-10);
}

// ===========================================================================
// 6. recalculate_mean_reward with various edge cases
// ===========================================================================

/// Empty reward list returns None.
#[test]
fn test_recalculate_mean_reward_empty() {
    assert!(identity::recalculate_mean_reward(&[]).is_none());
}

/// Single reward returns that value exactly.
#[test]
fn test_recalculate_mean_reward_single() {
    let refs = vec![RewardRef {
        value: 0.73,
        task_id: "t".to_string(),
        timestamp: "ts".to_string(),
        context_id: "c".to_string(),
    }];
    let avg = identity::recalculate_mean_reward(&refs).unwrap();
    assert!((avg - 0.73).abs() < 1e-10);
}

/// Two identical values return that value.
#[test]
fn test_recalculate_mean_reward_identical_values() {
    let refs = vec![
        RewardRef {
            value: 0.80,
            task_id: "t1".to_string(),
            timestamp: "ts1".to_string(),
            context_id: "c1".to_string(),
        },
        RewardRef {
            value: 0.80,
            task_id: "t2".to_string(),
            timestamp: "ts2".to_string(),
            context_id: "c2".to_string(),
        },
    ];
    let avg = identity::recalculate_mean_reward(&refs).unwrap();
    assert!((avg - 0.80).abs() < 1e-10);
}

/// Value of 0.0 averaged with 1.0 gives 0.5.
#[test]
fn test_recalculate_mean_reward_zero_and_one() {
    let refs = vec![
        RewardRef {
            value: 0.0,
            task_id: "t1".to_string(),
            timestamp: "ts".to_string(),
            context_id: "c".to_string(),
        },
        RewardRef {
            value: 1.0,
            task_id: "t2".to_string(),
            timestamp: "ts".to_string(),
            context_id: "c".to_string(),
        },
    ];
    let avg = identity::recalculate_mean_reward(&refs).unwrap();
    assert!((avg - 0.5).abs() < 1e-10);
}

/// Large number of rewards (100) computes correctly.
#[test]
fn test_recalculate_mean_reward_large_count() {
    let refs: Vec<RewardRef> = (0..100)
        .map(|i| RewardRef {
            value: (i as f64) / 99.0, // 0.0 to 1.0
            task_id: format!("t{}", i),
            timestamp: "ts".to_string(),
            context_id: "c".to_string(),
        })
        .collect();

    let avg = identity::recalculate_mean_reward(&refs).unwrap();
    // Sum of 0..99 / 99 = (99 * 100 / 2) / 99 = 50. Average = 50 / 100 = 0.5
    assert!(
        (avg - 0.5).abs() < 1e-10,
        "Average of 0.0..1.0 over 100 steps should be 0.5, got {}",
        avg
    );
}

/// Negative values are handled (no validation boundary).
#[test]
fn test_recalculate_mean_reward_with_negatives() {
    let refs = vec![
        RewardRef {
            value: -1.0,
            task_id: "t1".to_string(),
            timestamp: "ts".to_string(),
            context_id: "c".to_string(),
        },
        RewardRef {
            value: 1.0,
            task_id: "t2".to_string(),
            timestamp: "ts".to_string(),
            context_id: "c".to_string(),
        },
    ];
    let avg = identity::recalculate_mean_reward(&refs).unwrap();
    assert!((avg - 0.0).abs() < 1e-10);
}

/// All zeroes averages to zero.
#[test]
fn test_recalculate_mean_reward_all_zeros() {
    let refs: Vec<RewardRef> = (0..5)
        .map(|i| RewardRef {
            value: 0.0,
            task_id: format!("t{}", i),
            timestamp: "ts".to_string(),
            context_id: "c".to_string(),
        })
        .collect();

    let avg = identity::recalculate_mean_reward(&refs).unwrap();
    assert!((avg - 0.0).abs() < 1e-10);
}

/// All ones averages to one.
#[test]
fn test_recalculate_mean_reward_all_ones() {
    let refs: Vec<RewardRef> = (0..5)
        .map(|i| RewardRef {
            value: 1.0,
            task_id: format!("t{}", i),
            timestamp: "ts".to_string(),
            context_id: "c".to_string(),
        })
        .collect();

    let avg = identity::recalculate_mean_reward(&refs).unwrap();
    assert!((avg - 1.0).abs() < 1e-10);
}

/// Very small value differences are distinguishable.
#[test]
fn test_recalculate_mean_reward_precision() {
    let refs = vec![
        RewardRef {
            value: 0.333333333,
            task_id: "t1".to_string(),
            timestamp: "ts".to_string(),
            context_id: "c".to_string(),
        },
        RewardRef {
            value: 0.666666667,
            task_id: "t2".to_string(),
            timestamp: "ts".to_string(),
            context_id: "c".to_string(),
        },
    ];
    let avg = identity::recalculate_mean_reward(&refs).unwrap();
    assert!((avg - 0.5).abs() < 1e-8, "Expected ~0.5, got {}", avg);
}

/// update_performance correctly increments task_count and recalculates after each call.
#[test]
fn test_update_performance_sequential_correctness() {
    let mut record = RewardHistory {
        task_count: 0,
        mean_reward: None,
        rewards: vec![],
    };

    // After 1st update
    identity::update_performance(
        &mut record,
        RewardRef {
            value: 0.60,
            task_id: "t1".to_string(),
            timestamp: "ts".to_string(),
            context_id: "c".to_string(),
        },
    );
    assert_eq!(record.task_count, 1);
    assert!((record.mean_reward.unwrap() - 0.60).abs() < 1e-10);

    // After 2nd update: avg = (0.60 + 0.80) / 2 = 0.70
    identity::update_performance(
        &mut record,
        RewardRef {
            value: 0.80,
            task_id: "t2".to_string(),
            timestamp: "ts".to_string(),
            context_id: "c".to_string(),
        },
    );
    assert_eq!(record.task_count, 2);
    assert!((record.mean_reward.unwrap() - 0.70).abs() < 1e-10);

    // After 3rd update: avg = (0.60 + 0.80 + 1.0) / 3 = 0.80
    identity::update_performance(
        &mut record,
        RewardRef {
            value: 1.0,
            task_id: "t3".to_string(),
            timestamp: "ts".to_string(),
            context_id: "c".to_string(),
        },
    );
    assert_eq!(record.task_count, 3);
    assert!((record.mean_reward.unwrap() - 0.80).abs() < 1e-10);
}
